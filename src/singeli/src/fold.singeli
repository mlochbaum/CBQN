include './base'
if (hasarch{'AVX2'}) {
  include './sse'
  include './avx'
  include './avx2'
} else if (hasarch{'X86_64'}) {
  include './sse2'
} else if (hasarch{'AARCH64'}) {
  include './neon'
}
include './mask'

def opsh64{op}{v:[4]f64, perm} = op{v, shuf{[4]u64, v, perm}}
def opsh32{op}{v:[2]f64, perm} = op{v, shuf{[4]u32, v, perm}}
def mix{op, v:[4]f64 & hasarch{'AVX'}} = { def sh=opsh64{op}; sh{sh{v, 4b2301}, 4b1032} }
def mix{op, v:[2]f64 & hasarch{'X86_64'}} = opsh32{op}{v, 4b1032}

def reduce_pairwise{op, plog, x:*T, len, init:T} = {
  # Pairwise combination to shorten dependency chains
  def pairwise{p, i, k} = (if (k==0) { load{p,i} } else {
    def l = k-1
    op{pairwise{p, i       , l},
       pairwise{p, i+(1<<l), l}}
  })
  f:= len >> plog
  r:= init
  @for (i to f) r = op{r, pairwise{x+(i<<plog), 0, plog}}
  @for (x over i from f<<plog to len) r = op{r, x}
  r
}

fn fold_idem{T==f64, op}(x:*T, len:u64) : T = {
  def bulk = arch_defvw/width{T}
  def V = [bulk]T
  xv:= *V ~~ x
  r:V = [bulk]f64**0
  assert{len > 0}
  if (len<bulk) {
    # Can't overlap like the long case
    r = load{xv}
    if (bulk>2) {
      assert{(bulk==4) & hasarch{'AVX'}}
      if (len > 1) {
        if (len > 2) r = opsh64{op}{r, 4b2222}
                     r = opsh64{op}{r, 4b1111}
      }
    }
  } else {
    i:= load{*V ~~ (x+len-bulk)}
    r0:= reduce_pairwise{op, 2, xv, (len-1)/bulk, i}
    if (hasarch{'AARCH64'}) return{vfold{op, r0}}
    else r = mix{op, r0}
  }
  extract{r, 0}
}

export{'simd_fold_min_f64', fold_idem{f64,min}}
export{'simd_fold_max_f64', fold_idem{f64,max}}

fn fold_assoc_0{T==f64, op}(x:*T, len:u64) : T = {
  def bulk = arch_defvw/width{T}
  def V = [bulk]T
  xv:= *V ~~ x
  e:= len / bulk
  i:= load{xv, e} & (V~~maskOf{V, len % bulk})
  r:= reduce_pairwise{op, 2, xv, e, i}
  if (hasarch{'AARCH64'}) vfold{op, r}
  else extract{mix{op, r}, 0}
}
export{'simd_sum_f64', fold_assoc_0{f64,+}}

def insert_assoc_reg{T, op, id:T, x0:*void, len:u64, wid:u64} = {
  def n = 6
  def vw = arch_defvw / width{T}
  def V = [vw]T
  total := len*wid             # Total elements
  set := len; step := total; numr:u64 = 0
  if (total > n*vw) {
    set  = n*vw / wid          # Rows in one set of vectors
    assert{set > 0}
    step = set * wid           # Elements in one set
    numr = cdiv{total-n*vw, step} * step # Elements handled by main loop
  }

  # rv: masked load of up to n vectors starting at end (rem elements)
  x := *T~~x0; end := x + numr
  rem := total - numr
  rv  := undef{V, n}
  idv := V**id
  def id_labels = @collect (n) makelabel{}
  each{{g} => each{g, iota{n}, rv, id_labels}, tup{
    # Read and possibly mask
    {i, v, lbl} => {
      v = load{*V~~end, i}
      def j = i+1
      if (j==n or j*vw >= rem) {
        v = homBlend{idv, v, maskOf{V, rem-i*vw}}
        goto{lbl}
      }
    },
    # Set to identity
    {i,v,lbl} => { if (i>0) v=idv; setlabel{lbl} }
  }}

  while (x < end) {
    rv = each{{v,i} => op{v, load{*V~~x, i}}, rv, iota{n}}
    x += step
  }

  # In-vector reduction on shape set,wid
  mem:*T = ((n+1)*width{V} / 8) ** undef{T} # temporary memory
  def storeN{n} = each{{rv,i} => storeBatch{mem, i, rv, maskNone}, slice{rv,0,n}, iota{n}} # store first n registers
  def vnh = cdiv{2*n,3}>>0
  storeN{n}
  
  if (set>1) {
    crn:= set # current row count
    while(crn>1) {
      memhi:= mem + wid*(crn>>1)
      each{{a,i} => a=op{a, loadBatch{memhi,i,V}}, slice{rv,0,vnh}, iota{vnh}}
      storeN{vnh}
      crn = cdiv{crn,2}
    }
  }
  mem
}

fn insert_assoc{T, op, id0}(r:*void, x:*void, len:u64, w:u64) : void = {
  def id = if (ksym{id0}) emit{T, '', id0} else T~~id0
  def mem = insert_assoc_reg{T, op, id, x, len, w}
  emit{void, 'memcpy', r, mem, w*(width{T}/8)}
}

def maxvalue{T==f64} = 'INFINITY'
exportT{
  'avx2_insert_min',
  each{{T} => insert_assoc{T, min, maxvalue{T}}, tup{i8,i16,i32,f64}}
}
