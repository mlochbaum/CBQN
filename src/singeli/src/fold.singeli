include './base'
include './mask'

def opsh64{op}{v:[4]f64, perm} = op{v, shuf{[4]u64, v, perm}}
def opsh32{op}{v:[2]f64, perm} = op{v, shuf{[4]u32, v, perm}}
def mix{op, v:[4]f64 & hasarch{'AVX'}} = { def sh=opsh64{op}; sh{sh{v, 4b2301}, 4b1032} }
def mix{op, v:[2]f64 & hasarch{'X86_64'}} = opsh32{op}{v, 4b1032}

def reduce_pairwise{op, plog, x:*T, len, init:T} = {
  # Pairwise combination to shorten dependency chains
  def pairwise{p, i, k} = (if (k==0) { load{p,i} } else {
    def l = k-1
    op{pairwise{p, i       , l},
       pairwise{p, i+(1<<l), l}}
  })
  f:= len >> plog
  r:= init
  @for (i to f) r = op{r, pairwise{x+(i<<plog), 0, plog}}
  @for (x over i from f<<plog to len) r = op{r, x}
  r
}

fn fold_idem{T==f64, op}(x:*T, len:u64) : T = {
  r:T = load{x}; @for (x over _ from 1 to len) r = op{r, x}; r
}
fn fold_idem{T==f64, op & has_simd}(x:*T, len:u64) : T = {
  def bulk = arch_defvw/width{T}
  def V = [bulk]T
  xv:= *V ~~ x
  r:V = [bulk]f64**0
  assert{len > 0}
  if (len<bulk) {
    # Can't overlap like the long case
    r = load{xv}
    if (bulk>2) {
      assert{(bulk==4) & hasarch{'AVX'}}
      if (len > 1) {
        if (len > 2) r = opsh64{op}{r, 4b2222}
                     r = opsh64{op}{r, 4b1111}
      }
    }
  } else {
    i:= load{*V ~~ (x+len-bulk)}
    r0:= reduce_pairwise{op, 2, xv, (len-1)/bulk, i}
    if (hasarch{'AARCH64'}) return{vfold{op, r0}}
    else r = mix{op, r0}
  }
  extract{r, 0}
}

export{'simd_fold_min_f64', fold_idem{f64,min}}
export{'simd_fold_max_f64', fold_idem{f64,max}}

fn fold_assoc_0{T==f64, op}(x:*T, len:u64) : T = {
  r:T = 0; @for (x over len) r += x; r
}
fn fold_assoc_0{T==f64, op & has_simd}(x:*T, len:u64) : T = {
  def bulk = arch_defvw/width{T}
  def V = [bulk]T
  xv:= *V ~~ x
  e:= len / bulk
  i:= load{xv, e} & (V~~maskOf{V, len % bulk})
  r:= reduce_pairwise{op, 2, xv, e, i}
  if (hasarch{'AARCH64'}) vfold{op, r}
  else extract{mix{op, r}, 0}
}
export{'simd_sum_f64', fold_assoc_0{f64,+}}



def assoc_num_reg = 6
def insert_assoc_reg{T, op, id:T, x0:*void, len:u64, wid:u64} = {
  def n = assoc_num_reg
  def vw = arch_defvw / width{T}
  def V = [vw]T
  total := len*wid             # Total elements
  set := len; step := total; numr:u64 = 0
  if (total > n*vw) {
    set  = n*vw / wid          # Rows in one set of vectors
    assert{set > 0}
    step = set * wid           # Elements in one set
    numr = cdiv{total-n*vw, step} * step # Elements handled by main loop
  }

  # rv: masked load of up to n vectors starting at end (rem elements)
  x := *T~~x0; end := x + numr
  rem := total - numr
  rv  := undef{V, n}
  idv := V**id
  # Last index and corresponding masked vector
  li  := (rem-1)/vw
  last:= homBlend{idv, load{*V~~end, li}, maskOf{V, rem-li*vw}}
  def id_labels = @collect (n) makelabel{}
  each{each{., iota{n}, rv, id_labels}, tup{
    # Read or use last vector
    {i, v, lbl} => {
      if (i<n-1 and i<li) { v = load{*V~~end, i} }
      else                { v = last; goto{lbl} }
    },
    # Set to identity
    {i,v,lbl} => { if (i>0) v=idv; setlabel{lbl} }
  }}

  while (x < end) {
    rv = each{{v,i} => op{v, load{*V~~x, i}}, rv, iota{n}}
    x += step
  }

  def opI{a, b} = I~~op{V~~a, V~~b}
  ws := wid*(width{T}/8)
  insert_assoc_finish{opI, I~~idv, each{{v}=>{u:=I~~v},rv}, set, ws}
}

# Shuffle uses byte indices
def ls = 16; def S = [ls]i8                     # One lane
def ns = arch_defvw / width{i8}; def I = [ns]i8 # One vector
def has_lanes = ns>ls

def insert_assoc_finish{op, idv:I, rv, set:u64, ws:u64} = {
  def n = tuplen{rv}
  # In-vector reduction on shape set,wid
  def done = makelabel{}
  if (set > 1) {
    vms := I**(ls-1)
    hiota := make{I, iota{ns} % ls}
    ko := set*ws           # Number of bytes
    s := (set+1)/2         # Number of rows after halving
    k := s*ws              # Corresponding number of bytes
    def finish_step{} = {
      if (s==1) goto{done}
      ko = k
      s = (s+1)/2
      k = s*ws
    }
    while (k >= ls) {
      kc := ko - k  # Bytes on left hand side
      km := kc/ns; o := kc%ns
      kmask := maskOf{I, o}
      vo := I**cast_i{i8, o%ls}
      selmask := hiota < vo
      selind  := (hiota + vo) & vms
      def do_offset{m} = {
        def i = iota{min{2*(m+1), n-m}}  # Vectors to change
        {a, b} := tupsel{tup{i+m, i+m+1}, merge{rv, tup{idv}}}
        if (has_lanes) {
          h := each{shufHalves{., ., 16b21}, a,b}
          if (o<ls) b = h; else a = h
        }
        def shift_op{r, a,b, i} = {
          t := sel{S, homBlend{a, b, selmask}, selind}
          r = (if (i<=m) op{r, t} else t)
          if (i==m) r = homBlend{t, r, kmask}
        }
        each{shift_op, tupsel{i,rv}, a,b, i}
      }
      def do_offset{m==n/2} = {
        each{{r,t}=>{r = op{r,t}}, slice{rv,0,m}, slice{rv,m}}
      }
      def rec{m, max} = {
        if (km==m) do_offset{m}; else if (m<max) rec{m+1, max}
      }
      rec{1-has_lanes, cdiv{n,2}}
      finish_step{}
    }
    # Down to one vector; reduce to a lane
    def v = tupsel{0, rv}
    while (1) {
      mk:= maskOf{I, k}
      t := if (has_lanes) homBlend{v, shuf{[4]i64, v, 4b1032}, mk} else v
      o := op{v, sel{S, t, (hiota + I**cast_i{i8, k}) & vms}}
      v = homBlend{v, o, mk}
      finish_step{}
    }
  }
  setlabel{done}
  rv  # Caller writes
}

fn write_reg{n}(rp:*I, rv:n**I, bytes:u64) = {
  lastv := tupsel{0, rv}
  i:u64 = 0
  each{
    {v} => {
      if ((i+1)*ns >= bytes) goto{'last'}
      store{rp, i, lastv}
      ++i; lastv = v
    },
    slice{rv, 1}
  }
  setlabel{'last'}
  homMaskStoreF{rp+i, maskOf{I, bytes-i*ns}, lastv}
}

fn insert_assoc{T, op, id0}(r0:*void, x0:*void, len:u64, wid:u64) : void = {
  assert{len > 1}
  if (not hasarch{'AVX2'}) {
    r := *T~~r0; x := *T~~x0
    s := x  # Source; don't copy first row
    @for (i from 1 to len) {
      x += wid
      @for (r, s, x over wid) r = op{s, x}
      s = r
    }
  } else {
    def vw = arch_defvw / width{T}
    def V = [vw]T
    if (wid <= assoc_num_reg * vw) {
      def id = if (ksym{id0}) emit{T, '', id0} else T~~id0
      def rv = insert_assoc_reg{T, op, id, x0, len, wid}
      write_reg{tuplen{rv}}(*I~~r0, rv, wid*(width{T}/8))
    } else {
      # Accumulate columns of n registers
      def n = 4; def nvw = n*vw; def i = iota{n}
      r := *T~~r0; xc := *T~~x0
      end := xc + len*wid
      last := xc + wid - nvw
      while (1) {
        rv := each{load{*V~~xc, .}, i}
        x := xc + wid
        while (x < end) {
          rv = each{{v,i} => op{v, load{*V~~x, i}}, rv, i}
          x += wid
        }
        each{store{*V~~r, ...}, i, rv}
        xc += nvw; r += nvw
        if (rare{xc > last}) {
          if (xc == last+nvw) return{}
          xc = last
          r = *T~~r0 + (wid - nvw)
        }
      }
    }
  }
}

def maxvalue{T==f64} = 1/0
exportT{
  'simd_insert_min',
  each{{T} => insert_assoc{T, min, maxvalue{T}}, tup{i8,i16,i32,f64}}
}
