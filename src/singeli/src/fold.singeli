include './base'
include './mask'

def opsh64{op}{v:[4]f64, perm} = op{v, shuf{[4]u64, v, perm}}
def opsh32{op}{v:[2]f64, perm} = op{v, shuf{[4]u32, v, perm}}
def mix{op, v:[4]f64 & hasarch{'AVX'}} = { def sh=opsh64{op}; sh{sh{v, 4b2301}, 4b1032} }
def mix{op, v:[2]f64 & hasarch{'X86_64'}} = opsh32{op}{v, 4b1032}

def reduce_pairwise{op, plog, x:*T, len, init:T} = {
  # Pairwise combination to shorten dependency chains
  def pairwise{p, i, k} = (if (k==0) { load{p,i} } else {
    def l = k-1
    op{pairwise{p, i       , l},
       pairwise{p, i+(1<<l), l}}
  })
  f:= len >> plog
  r:= init
  @for (i to f) r = op{r, pairwise{x+(i<<plog), 0, plog}}
  @for (x over i from f<<plog to len) r = op{r, x}
  r
}

fn fold_idem{T==f64, op}(x:*T, len:u64) : T = {
  def bulk = arch_defvw/width{T}
  def V = [bulk]T
  xv:= *V ~~ x
  r:V = [bulk]f64**0
  assert{len > 0}
  if (len<bulk) {
    # Can't overlap like the long case
    r = load{xv}
    if (bulk>2) {
      assert{(bulk==4) & hasarch{'AVX'}}
      if (len > 1) {
        if (len > 2) r = opsh64{op}{r, 4b2222}
                     r = opsh64{op}{r, 4b1111}
      }
    }
  } else {
    i:= load{*V ~~ (x+len-bulk)}
    r0:= reduce_pairwise{op, 2, xv, (len-1)/bulk, i}
    if (hasarch{'AARCH64'}) return{vfold{op, r0}}
    else r = mix{op, r0}
  }
  extract{r, 0}
}

export{'simd_fold_min_f64', fold_idem{f64,min}}
export{'simd_fold_max_f64', fold_idem{f64,max}}

fn fold_assoc_0{T==f64, op}(x:*T, len:u64) : T = {
  def bulk = arch_defvw/width{T}
  def V = [bulk]T
  xv:= *V ~~ x
  e:= len / bulk
  i:= load{xv, e} & (V~~maskOf{V, len % bulk})
  r:= reduce_pairwise{op, 2, xv, e, i}
  if (hasarch{'AARCH64'}) vfold{op, r}
  else extract{mix{op, r}, 0}
}
export{'simd_sum_f64', fold_assoc_0{f64,+}}

def insert_assoc_reg{T, op, id:T, x0:*void, len:u64, wid:u64} = {
  def n = 6
  def vw = arch_defvw / width{T}
  def V = [vw]T
  total := len*wid             # Total elements
  set := len; step := total; numr:u64 = 0
  if (total > n*vw) {
    set  = n*vw / wid          # Rows in one set of vectors
    assert{set > 0}
    step = set * wid           # Elements in one set
    numr = cdiv{total-n*vw, step} * step # Elements handled by main loop
  }

  # rv: masked load of up to n vectors starting at end (rem elements)
  x := *T~~x0; end := x + numr
  rem := total - numr
  rv  := undef{V, n}
  idv := V**id
  # Last index and corresponding masked vector
  li  := (rem-1)/vw
  last:= homBlend{idv, load{*V~~end, li}, maskOf{V, rem-li*vw}}
  def id_labels = @collect (n) makelabel{}
  each{{g} => each{g, iota{n}, rv, id_labels}, tup{
    # Read or use last vector
    {i, v, lbl} => {
      if (i<n-1 and i<li) { v = load{*V~~end, i} }
      else                { v = last; goto{lbl} }
    },
    # Set to identity
    {i,v,lbl} => { if (i>0) v=idv; setlabel{lbl} }
  }}

  while (x < end) {
    rv = each{{v,i} => op{v, load{*V~~x, i}}, rv, iota{n}}
    x += step
  }

  def opI{a, b} = I~~op{V~~a, V~~b}
  ws := wid*(width{T}/8)
  insert_assoc_finish{opI, I~~idv, each{{v}=>{u:=I~~v},rv}, set, ws}
}

# Shuffle uses byte indices
def ls = 16; def ms = ls-1; def S = [ls]i8      # One lane
def ns = arch_defvw / width{i8}; def I = [ns]i8 # One vector
def has_lanes = ns>ls

def insert_assoc_finish{op, idv:I, rv, set:u64, ws:u64} = {
  def n = tuplen{rv}
  # In-vector reduction on shape set,wid
  def done = makelabel{}
  if (set > 1) {
    vms := I**ms
    viota := make{I, iota{ns}}
    hiota := viota & vms
    s := (set+1)/2         # Number of rows after halving
    k := s*ws              # Corresponding number of bytes
    def finish_step{} = {
      if (s==1) goto{done}
      set = s
      s = (s+1)/2
      k = s*ws
    }
    while (k >= ls) {
      km := k/ns; o := k%ns
      vo := I**cast_i{i8, o & ms}
      selmask := hiota < vo
      selind  := (hiota + vo) & vms
      def set_shift{r,a,b} = {
        r = op{r, sel{S, homBlend{a, b, selmask}, selind}}
      }
      sws := set*ws
      def do_offset{m} = {
        def nv = min{m+1, n-m}  # Number of vectors to change
        @unroll (i from m+(m+1)>>1 to min{m+nv, n}) {
          def v = tupsel{i, rv}
          v = homBlend{idv, v, viota < I**cast_i{i8,sws-i*ns}}
        }
        def i = iota{nv}
        {a, b} := tupsel{tup{i+m, i+m+1}, merge{rv, tup{idv}}}
        if (has_lanes) {
          h := each{{a,b}=>shufHalves{a,b, 16b21}, a,b}
          if (o<ls) b = h; else a = h
        }
        def nsh = min{n_share,nv}
        def sl_e{vs, i,j} = each{{v}=>slice{v, i,j}, vs}
        each{set_shift, ...sl_e{tup{rv,a,b}, nsh,nv}}
        sl_e{ab_share, 0,nsh} = sl_e{tup{a,b}, 0,nsh}
      }
      def rec{m, max} = {
        if (km==m) do_offset{m}; else if (m<max) rec{m+1, max}
      }
      def n_share = 1
      ab_share := 2**n_share**idv
      rec{1-has_lanes, cdiv{2*n,3}}
      each{set_shift, slice{rv,0,n_share}, ...ab_share}
      finish_step{}
    }
    # Down to one vector; reduce to a lane
    def v = tupsel{0, rv}
    while (1) {
      v = homBlend{idv, v, I~~maskOf{I, set*ws}}
      t := if (has_lanes) homBlend{v, shuf{[4]i64, v, 4b1032}, maskOf{I, k}} else v
      v = op{v, sel{S, t, (hiota + I**cast_i{i8, k}) & vms}}
      finish_step{}
    }
  }
  setlabel{done}
  rv  # Caller writes
}

fn write_reg{n}(rp:*I, rv:n**I, bytes:u64) = {
  lastv := tupsel{0, rv}
  i:u64 = 0
  each{
    {v} => {
      if ((i+1)*ns >= bytes) goto{'last'}
      store{rp, i, lastv}
      ++i; lastv = v
    },
    slice{rv, 1}
  }
  setlabel{'last'}
  homMaskStoreF{rp+i, maskOf{I, bytes-i*ns}, lastv}
}

fn insert_assoc{T, op, id0}(r:*void, x:*void, len:u64, wid:u64) : void = {
  def id = if (ksym{id0}) emit{T, '', id0} else T~~id0
  def rv = insert_assoc_reg{T, op, id, x, len, wid}
  write_reg{tuplen{rv}}(*I~~r, rv, wid*(width{T}/8))
}

def maxvalue{T==f64} = 'INFINITY'
exportT{
  'avx2_insert_min',
  each{{T} => insert_assoc{T, min, maxvalue{T}}, tup{i8,i16,i32,f64}}
}
